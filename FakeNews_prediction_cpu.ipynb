{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae96bddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 08:00:11.716651: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-08 08:00:11.800481: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-08 08:00:11.800497: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-08 08:00:12.269283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-08 08:00:12.269373: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-08 08:00:12.269381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sys\n",
    "import csv\n",
    "import seaborn as sb\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from bertopic import BERTopic\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08692a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('newest_test_data', engine='python', encoding='utf-8', on_bad_lines='skip').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e6e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfe1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    labels = []\n",
    "    topics = []\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    sentiment_score = []\n",
    "    for index, line in data.iterrows():\n",
    "        labels.append(int(line.label))\n",
    "        topics.append(int(line.topic))\n",
    "        sentiment_score.append(float(line.text_sentiment_score))\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            line.idf_words,\n",
    "                            add_special_tokens = True, # [CLS] & [SEP]\n",
    "                            truncation = 'longest_first', # Control truncation\n",
    "                            max_length = 100, # Max length about texts\n",
    "                            pad_to_max_length = True, # Pad and truncate about sentences\n",
    "                            return_attention_mask = True, # Attention masks\n",
    "                            return_tensors = 'pt') # Return to pytorch tensors\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    try:\n",
    "        labels = torch.tensor(labels)\n",
    "    except:\n",
    "        print(labels)\n",
    "\n",
    "    topics = torch.tensor(topics)\n",
    "    sentiment_score = torch.tensor(sentiment_score)\n",
    "    return input_ids, attention_masks, labels, topics, sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dbc6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPlusModel(torch.nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertPlusModel, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.linear = torch.nn.Linear(769, 1)  # The output of BERT has size 768, so the input size to the linear layer should be 768\n",
    "        self.sigmoid = torch.nn.Sigmoid()  # The sigmoid function will map the output to the range [0, 1]\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, additional_input):\n",
    "        _, pooled_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask).to_tuple()\n",
    "        concatenated = torch.cat((pooled_output, additional_input.unsqueeze(dim=1)), dim=1)\n",
    "        return self.linear(concatenated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006a1d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertPlusModel(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=769, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the pretrained BERT model\n",
    "bert_model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize the model with the pretrained BERT model\n",
    "model = BertPlusModel(bert_model)\n",
    "\n",
    "# Load model CPU\n",
    "model.load_state_dict(torch.load('model_10epoch_8batch.pt', map_location='cpu'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3734dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_id(article):\n",
    "     for index, line in article.iterrows():\n",
    "        return int(line.topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406ad7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_artciles_with_same_topic(article, dataset):\n",
    "   topic = get_topic_id(article)\n",
    "   filtered_articles = dataset.loc[dataset['topic'] == topic] \n",
    "   return filtered_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "401d1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_articles_based_on_predictions(articles, predictions, RECOMMENDATION_SIZE):\n",
    "    result = []\n",
    "    recommendations_found = 0\n",
    "    for article_title, article_text, prediction in zip(articles['title'], articles['text'], predictions):\n",
    "        if(prediction == 1):\n",
    "            result.append([article_title, article_text])\n",
    "            recommendations_found += 1\n",
    "            if(recommendations_found >= RECOMMENDATION_SIZE):\n",
    "                return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd419f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(query):\n",
    "    query_input_ids, query_attention_masks, query_labels, query_topics, query_sentiment_score = tokenize_data(query)\n",
    "    \n",
    "    # Move tensors to GPU\n",
    "#     query_input_ids = query_input_ids.to(device)\n",
    "#     query_attention_masks = query_attention_masks.to(device)\n",
    "#     query_labels = query_labels.to(device)\n",
    "#     query_topics = query_topics.to(device)\n",
    "#     query_sentiment_score = query_sentiment_score.to(device)\n",
    "    \n",
    "    output = model(query_input_ids, query_attention_masks, query_topics)\n",
    "    query_labels = query_labels.type(torch.float).unsqueeze(1)\n",
    "    output = torch.round(torch.sigmoid(output))  \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbba1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "16\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "[['DR GORKA EXPOSES THE COMMIES: One Word Susan Rice Cronies Used Is A Warning To Americans [Video]', 'The weaponization of the security services is what you expect in a banana republic and a police state or authoritarian state. And let s just look into one more piece of evidence. These individuals are politically appointed. I am politically appointed. I served as a commissioned officer of the president. So did she. We understand that. But you re supposed to be a professional I challenge your viewers right now to go to her twitter feed and read her tweets after January 20th and read her colleagues  Ben Rhodes and Colin Kahl. You will see who these people really are Just three weeks ago Colin Kahl used the word purge Purging is the kind of word Maoists use. These people have unmasked themselves PURGE Susan Rice and her cronies only want to destroy the Trump administration. Maoists all '], ['Daniel Greenfield: Why Obama Really Spied On Trump: ‘It’s not just ideology. It’s raw fear.’', 'Daniel Greenfield s take on why Obama spied on Trump is the best yet. He takes you through the twists and turns to expose Obama and his cronies for the liars and cheats they are. We d love to know what you think about this take on why things happened the way they did:Last week, CNN revealed (and excused) one phase of the Obama spying operation on Trump. After lying about it on MSNBC, Susan Rice admitted unmasking the identities of Trump officials to Congress.ACKNOWLEDGED IN A LETTER FROM NUNES AND GOWDY:Rice was unmasking the names of Trump officials a month before leaving office. The targets may have included her own successor, General Flynn, who was forced out of office using leaked surveillance.SUSAN RICE LYING ABOUT THE UNMASKING   IS ANYONE SURPRISED? I know nothing about this. I was surprised to see reports from Chairman Nunes on that count today. What she says is a lie but who is surprised by this? We know that Susan Rice lied 5 times on 5 different morning shows the morning after Benghazi. Why wouldn t she try and cover this spying up to protect herself and others including Obama.Susan Rice is also giving conflicting stories on what she did so it might be a good idea for her to lawyer up right now. She claimed ignorance of the unmasking and spying but today she spoke about doing it. Yes, red flags are everywhere on this! The reality and truth is this was more of a political attack to destabilize the Trump presidency and embarrass him:Andrew McCarthy said it best: The national-security adviser is not an investigator. She is a White House staffer. The president s staff is a consumer of intelligence, not a generator or collector of it. If Susan Rice was unmasking Americans, it was not to fulfill an intelligence need based on American interests; it was to fulfill a political desire based on Democratic-party interests. GREENFIELD:  LAWS WERE BROKEN The bottom line is that laws were broken when the names were unmasked Someone s in BIG trouble! While Rice s targets weren t named, the CNN story listed a meeting with Flynn, Bannon and Kushner.Bannon was Trump s former campaign chief executive and a senior adviser. Kushner is a senior adviser. Those are exactly the people you spy on to get an insight into what your political opponents plan to do.Now the latest CNN spin piece informs us that secret FISA orders were used to spy on the conversations of Trump s former campaign chairman, Paul Manafort (SEE CLAPPER VIDEO BELOW). The surveillance was discontinued for lack of evidence and then renewed under a new warrant. This is part of a pattern of FISA abuses by Obama Inc. which never allowed minor matters like lack of evidence to dissuade them from new FISA requests. Its possible  President Trump s voice was picked up in a wiretap of Paul Manafort, says former spy chief Clapper https://t.co/2ELM48axAa  CNN (@CNN) September 21, 2017Desperate Obama cronies had figured out that they could bypass many of the limitations on the conventional investigations of their political opponents by  laundering  them through national security.If any of Trump s people were talking to non-Americans, the Foreign Intelligence Surveillance Act (FISA) could be used to spy on them. And then the redacted names of the Americans could be unmasked by Susan Rice, Samantha Power and other Obama allies. It was a technically legal Watergate.If both CNN stories hold up, then Obama Inc. had spied on two Trump campaign leaders.Furthermore the Obama espionage operation closely tracked Trump s political progress. The first FISA request targeting Trump happened the month after he received the GOP nomination. The second one came through in October: the traditional month of political surprises meant to upend an election.The spying ramped up after Trump s win when the results could no longer be used to engineer a Hillary victory, but would instead have to be used to cripple and bring down President Trump. Headed out the door, Rice was still unmasking the names of Trump s people while Obama was making it easier to pass around raw eavesdropped data to other agencies.Obama had switched from spying on a political opponent to win an election, to spying on his successor to undo the results of the election.RICE AND POWER WERE USED TO  UNMASK  TRUMP ASSOCIATES    ALL THE PRESIDENT S WOMEN? Notice how uncomfortable Brennan is when asked about the Ambassador Power:Abuse of power by a sitting government had become subversion of the government by an outgoing administration. Domestic spying on opponents had become a coup.The Democrat scandals of the past few administrations have hinged on gross violations of political norms, elementary ethics and the rule of law that, out of context, were not technically illegal.But it s the pattern that makes the crime. It s the context that shows the motive.Obama Inc. compartmentalized its espionage operation in individual acts of surveillance and unmasking, and general policies implemented to aid both, that may have been individually legal, in the purely technical sense, in order to commit the major crime of eavesdropping on the political opposition.When the individual acts of surveillance are described as legal, that s irrelevant. It s the collective pattern of surveillance of the political opposition that exposes the criminal motive for them.If Obama spied on two of Trump s campaign leaders, that s not a coincidence. It s a pattern.A criminal motive can be spotted by a consistent pattern of actions disguised by different pretexts. A dirty cop may lose two pieces of evidence from the same defendant while giving two different excuses. A shady accountant may explain two otherwise identical losses in two different ways. Both excuses are technically plausible. But it s the pattern that makes the crime.Manafort was spied on under the Russia pretext. Bannon may have been spied on over the UAE. That s two different countries, two different people and two different pretexts.But one single target. President Trump.It s the pattern that exposes the motive.When we learn the whole truth (if we ever do), we will likely discover that Obama Inc. assembled a motley collection of different technically legal pretexts to spy on Trump s team.Each individual pretext might be technically defensible. But together they add up to the crime of the century.Obama s gamble was that the illegal surveillance would justify itself. If you spy on a bunch of people long enough, especially people in politics and business, some sort of illegality, actual or technical, is bound to turn up. That s the same gamble anyone engaged in illegal surveillance makes.Businessmen illegally tape conversations with former partners hoping that they ll say something damning enough to justify the risk. That was what Obama and his allies were doing with Trump.It s a crime. And you can t justify committing a crime by discovering a crime.If everyone were being spied on all the time, many crimes could be exposed every second. But that s not how our system works. That s why we have a Fourth Amendment.Nor was Obama Inc. trying to expose crimes for their own sake, but to bring down the opposition.That s why it doesn t matter what results the Obama surveillance turned up. The surveillance was a crime. Anything turned up by it is the fruit of a poisonous tree. It s inherently illegitimate.The first and foremost agenda must be to assemble a list of Trump officials who were spied on and the pretexts under which they were spied upon. The pattern will show the crime. And that s what Obama and his allies are terrified of. It s why Flynn was forced out using illegal surveillance and leaks. It s why McMaster is protecting Susan Rice and the Obama holdovers while purging Trump loyalists at the NSC.The left s gamble was that the Mueller investigation or some other illegitimate spawn of the Obama eavesdropping would produce an indictment and then the procedural questions wouldn t matter.It s the dirty cop using illegal eavesdropping to generate leads for a  clean  case against his target while betting that no one will look too closely or care how the case was generated. If one of the Mueller targets is intimidated into making a deal, the question of how the case was generated won t matter.Mueller will have a cooperative witness. And the Democrats can begin their coup in earnest. It will eventually turn out that there is no  there  there. But by then, it ll be time for President Booker.There s just one problem.If the gamble fails, if no criminal case that amounts to anything more than the usual investigational gimmick charges like perjury (the Federal equivalent of  resisting arrest  for a beat cop) develops, then Obama and his allies are on the hook for the domestic surveillance of their political opponents.With nothing to show for it and no way to distract from it.That s the race against the clock that is happening right now. Either the investigation gets results. Or its perpetrators are left hanging in the wind. If McMaster is fired, which on purely statistical grounds he probably will be, and a Trump loyalist who wasn t targeted by the surveillance operation becomes the next National Security Adviser and brings in Trump loyalists, as Flynn tried to do, then it s over.And the Dems finally get their Watergate. Except the star won t be Trump, it will be Obama. Rice, Power, Lynch and the rest of the gang will be the new Haldeman, Ehrlichman and Mitchell.Once Obama and his allies launched their domestic surveillance operation, they crossed the Rubicon. And there was no way back. They had to destroy President Trump or risk going to jail.The more crimes they committed by spying on the opposition, the more urgently they needed to bring down Trump. The consequences of each crime that they had committed spurred them on to commit worse crimes to save themselves from going to jail. It s the same old story when it comes to criminals.Each act of illegal surveillance became more blatant. And when illegal surveillance couldn t stop Trump s victory, they had to double down on the illegal surveillance for a coup.The more Obama spied on Trump, the more he had to keep doing it. This time it was bound to pay off.Obama and his allies had violated the norms so often for their policy goals that they couldn t afford to be replaced by anyone but one of their own. The more Obama relied on the imperial presidency of executive orders, the less he could afford to be replaced by anyone who would undo them. The more his staffers lied and broke the law on everything from the government shutdown to the Iran nuke sellout, the more desperately they needed to pull out all the stops to keep Trump out of office. And the more they did it, the more they couldn t afford not to do it. Abuse of power locks you into the loop familiar to all dictators. You can t stop riding the tiger. Once you start, you can t afford to stop.If you want to understand why Samantha Power was unmasking names, that s why. The hysterical obsession with destroying Trump comes from the top down. It s not just ideology. It s wealthy and powerful men and women who ran the country and are terrified that their crimes will be exposed.It s why the media increasingly sounds like the propaganda organs of a Communist country. Why there are street riots and why the internet is being censored by Google and Facebook s  fact checking  allies.It s not just ideology. It s raw fear.The left is sitting on the biggest crime committed by a sitting president. The only way to cover it up is to destroy his Republican successor.A turning point in history is here.If Obama goes down, the left will go down with him. If his coup succeeds, then America ends.Read more from Daniel Greenfield'], ['11 Highlights of Susan Rice’s MSNBC Interview with Andrea Mitchell - Breitbart', 'On Tuesday afternoon, President Barack Obama’s former National Security Advisor Susan Rice appeared on MSNBC with host Andrea Mitchell to answer questions about allegations that had emerged earlier in the week to suggest that she requested the “unmasking” of the names of Donald Trump’s campaign and transition teams in intelligence reports, which allegedly had nothing to do with national security, and that she had compiled spreadsheets of those names. [Here are the highlights of Mitchell’s interview with Rice, which took up the first   of Mitchell’s show.  Joel B. Pollak is Senior    at Breitbart News. He was named one of the “most influential” people in news media in 2016. His new book, How Trump Won: The Inside Story of a Revolution, is available from Regnery. Follow him on Twitter at @joelpollak.'], ['Bolton on Susan Rice Scandal: Obama Needs to Be Asked What He Knew and When He Knew It', 'On Thursday’s Breitbart News Daily, SiriusXM host Raheem Kassam asked John Bolton about allegations that former National Security Adviser Susan Rice abused intelligence resources to spy on President Obama’s political opponents, including Donald Trump’s 2016 presidential campaign. [“I think she absolutely deserves the opportunity to clear her name, under oath, before several congressional committees,” Bolton responded. “And probably so do a lot of other people in the Obama administration, right up to and including the president. ”  “You know the famous question that Sen. Howard Baker asked repeatedly during the Watergate hearings: ‘What did the president know, and when did he know it?’ That question needs to be put to Barack Obama,” he recommended. John Bolton is a former U. N. ambassador, senior fellow at the American Enterprise Institute, and head of the BoltonPAC political action committee. Breitbart News Daily airs on SiriusXM Patriot 125 weekdays from 6:00 a. m. to 9:00 a. m. Eastern. LISTEN: '], ['Rand Paul: Susan Rice ’Ought to Be Under Subpoena,’ Asked If Obama Knew About Eavesdropping - Breitbart', 'Tuesday on MSNBC’s “Morning Joe,” Sen. Rand Paul ( ) called on former National Security Advisor Susan Rice to be brought in front of Congress under subpoena and asked questions about allegations she was behind the unmasking of American identities in raw surveillance.  Paul also said she should be asked about former President Barack Obama’s knowledge of these alleged activities. “For years, both progressives and libertarians have been complaining about these backdoor searches,” Paul said. “It’s not that we’re searching maybe one foreign leader and who they talk to we search everything in the whole world. There were reports a couple of years ago that all of Italy’s phone calls were absorbed in a one month period of time. We were getting Merkel’s phone calls we were getting everybody’s phone calls. But by rebound we are collecting millions of Americans phone calls. If you want to look at an American’s phone call or listen to it, you should have to have a warrant, the old fashioned way in a real court where both sides get represented. ” “But a secret warrant by a secret court with a lower standard level because we’re afraid of terrorism is one thing for foreigners but both myself and a Progressive Ron Wyden have been warning about these back door searches for years and that they could be politicized,” he continued. “The facts will come out with Susan Rice. But I think she ought to be under subpoena. She should be asked did you talk to the president about it? Did President Obama know about this? So this is actually, eerily similar to what Trump accused them of which is eavesdropping on conversations for political reasons. ” Follow Jeff Poor on Twitter @jeff_poor']]\n"
     ]
    }
   ],
   "source": [
    "RECOMMENDATION_SIZE = 5\n",
    "\n",
    "query = test_data.sample(n=1).reset_index(drop=True)\n",
    "\n",
    "print(get_topic_id(query))\n",
    "\n",
    "articles_with_same_topic = filter_artciles_with_same_topic(query, test_data)\n",
    "print(len(articles_with_same_topic))\n",
    "predictions = predict_label(articles_with_same_topic).squeeze(1)\n",
    "print(predictions)\n",
    "\n",
    "recommended_articles = filter_articles_based_on_predictions(articles_with_same_topic, predictions, RECOMMENDATION_SIZE)\n",
    "\n",
    "print(recommended_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee63e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
