{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 21:01:46.864689: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-07 21:01:46.961028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-07 21:01:46.961044: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-07 21:01:47.417984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-07 21:01:47.418117: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-07 21:01:47.418127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sys\n",
    "import csv\n",
    "import seaborn as sb\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from bertopic import BERTopic\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('newest_train_data', engine='python', encoding='utf-8', on_bad_lines='skip').fillna('')\n",
    "train_data = train_data[pd.to_numeric(train_data['text_sentiment_score'], errors='coerce').notnull()]\n",
    "#train_data = train_data.sample(n=1000).reset_index(drop=True)\n",
    "\n",
    "test_data = pd.read_csv('newest_test_data', engine='python', encoding='utf-8', on_bad_lines='skip').fillna('')\n",
    "#test_data = test_data.sample(n=1000).reset_index(drop=True)\n",
    "\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    labels = []\n",
    "    topics = []\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    sentiment_score = []\n",
    "    for index, line in data.iterrows():\n",
    "        labels.append(int(line.label))\n",
    "        topics.append(int(line.topic))\n",
    "        sentiment_score.append(float(line.text_sentiment_score))\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            line.idf_words,\n",
    "                            add_special_tokens = True, # [CLS] & [SEP]\n",
    "                            truncation = 'longest_first', # Control truncation\n",
    "                            max_length = 100, # Max length about texts\n",
    "                            pad_to_max_length = True, # Pad and truncate about sentences\n",
    "                            return_attention_mask = True, # Attention masks\n",
    "                            return_tensors = 'pt') # Return to pytorch tensors\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    try:\n",
    "        labels = torch.tensor(labels)\n",
    "    except:\n",
    "        print(labels)\n",
    "\n",
    "    topics = torch.tensor(topics)\n",
    "    sentiment_score = torch.tensor(sentiment_score)\n",
    "    return input_ids, attention_masks, labels, topics, sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids, test_attention_masks, test_labels, test_topics, test_sentiment_score = tokenize_data(test_data)\n",
    "train_input_ids, train_attention_masks, train_labels, train_topics, train_sentiment_score = tokenize_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57705, 14427)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare GPU as device to run on\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Move tensors to GPU\n",
    "test_input_ids = test_input_ids.to(device)\n",
    "test_attention_masks = test_attention_masks.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "test_topics = test_topics.to(device)\n",
    "test_sentiment_score = test_sentiment_score.to(device)\n",
    "\n",
    "train_input_ids = train_input_ids.to(device)\n",
    "train_attention_masks = train_attention_masks.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "train_topics = train_topics.to(device)\n",
    "train_sentiment_score = train_sentiment_score.to(device)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels, train_topics,train_sentiment_score)\n",
    "\n",
    "val_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels, test_topics, test_sentiment_score)\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "#num_workers = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "                dataset=train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=True)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "                dataset=val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT + TOPIC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class BertPlusModel(torch.nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertPlusModel, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.linear = torch.nn.Linear(769, 1)  # The output of BERT has size 768, so the input size to the linear layer should be 768\n",
    "        self.sigmoid = torch.nn.Sigmoid()  # The sigmoid function will map the output to the range [0, 1]\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, additional_input):\n",
    "        _, pooled_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask).to_tuple()\n",
    "        concatenated = torch.cat((pooled_output, additional_input.unsqueeze(dim=1)), dim=1)\n",
    "        return self.linear(concatenated)\n",
    "\n",
    "# Get the pretrained BERT model\n",
    "bert_model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize the model with the pretrained BERT model\n",
    "model = BertPlusModel(bert_model)\n",
    "\n",
    "# Load model instead of training\n",
    "#model.to('cuda')\n",
    "#model.load_state_dict(torch.load('model.pt', map_location='cuda'))\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 7213/7213 [30:08<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.0388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 7213/7213 [30:06<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 7213/7213 [30:01<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 7213/7213 [30:00<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 7213/7213 [30:19<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 7213/7213 [30:14<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 7213/7213 [30:05<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 7213/7213 [30:08<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 7213/7213 [30:20<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 7213/7213 [29:41<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.0868\n"
     ]
    }
   ],
   "source": [
    "model.train()  \n",
    "total_step = len(train_dataloader.dataset)\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader := tqdm(train_dataloader, total=len(train_dataloader))):\n",
    "        # Unpack the input and label tensors from the batch\n",
    "        input_ids, attention_masks, labels, topics, sentiment_scores = batch\n",
    "        \n",
    "        output = model(input_ids, attention_masks, topics)\n",
    "        \n",
    "        \n",
    "        \n",
    "        labels = labels.type(torch.float).unsqueeze(1)\n",
    "\n",
    "        # Calculate the loss between the output and the labels\n",
    "        loss = loss_fn(output, labels)\n",
    "        \n",
    "        # Backpropagate the loss and update the model's parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print('Epoch: {}, Loss: {:.4f}'.format(epoch+1, total_loss / total_step))\n",
    "\n",
    "torch.save(model.state_dict(), \"model_10epoch_8batch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, val_loss: 0.6921, val_acc: 0.5393\n",
      "epoch: 1, val_loss: 0.6921, val_acc: 0.5393\n",
      "epoch: 2, val_loss: 0.6921, val_acc: 0.5393\n",
      "epoch: 3, val_loss: 0.6920, val_acc: 0.5393\n",
      "epoch: 4, val_loss: 0.6920, val_acc: 0.5393\n",
      "epoch: 5, val_loss: 0.6920, val_acc: 0.5393\n",
      "epoch: 6, val_loss: 0.6921, val_acc: 0.5393\n",
      "epoch: 7, val_loss: 0.6921, val_acc: 0.5393\n",
      "epoch: 8, val_loss: 0.6920, val_acc: 0.5393\n",
      "epoch: 9, val_loss: 0.6920, val_acc: 0.5393\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "num_correct = 0\n",
    "num_samples = 0\n",
    "    \n",
    "for epoch in range(10):\n",
    "    for batch in val_dataloader:\n",
    "            # Unpack the input and label tensors from the batch\n",
    "            input_ids, attention_masks, labels, topics, sentiment_scores = batch\n",
    "       \n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids, attention_masks, topics)\n",
    "                labels = labels.type(torch.float).unsqueeze(1)\n",
    "                loss = loss_fn(output,  labels)\n",
    "            \n",
    "            # Calculate the loss between the output and the labels\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            num_samples += input_ids.size(0)\n",
    "            \n",
    "            output = torch.round(torch.sigmoid(output))\n",
    "            correct_entries = torch.eq(output, labels)\n",
    "            num_correct += torch.sum(correct_entries)\n",
    "        \n",
    "    print(f'epoch: {epoch}, val_loss: {total_loss / num_samples:.4f}, val_acc: {num_correct / num_samples:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_id(article):\n",
    "     for index, line in article.iterrows():\n",
    "        return int(line.topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_artciles_with_same_topic(article, dataset):\n",
    "   topic = get_topic_id(article)\n",
    "   filtered_articles = dataset.loc[dataset['topic'] == topic] \n",
    "   return filtered_articles\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_articles_based_on_predictions(articles, predictions, RECOMMENDATION_SIZE):\n",
    "    result = []\n",
    "    recommendations_found = 0\n",
    "    for article_title, article_text, prediction in zip(articles['title'], articles['text'], predictions):\n",
    "        if(prediction == 1):\n",
    "            result.append([article_title, article_text])\n",
    "            recommendations_found += 1\n",
    "            if(recommendations_found >= RECOMMENDATION_SIZE):\n",
    "                return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(query):\n",
    "    query_input_ids, query_attention_masks, query_labels, query_topics, query_sentiment_score = tokenize_data(query)\n",
    "    \n",
    "    # Move tensors to GPU\n",
    "    query_input_ids = query_input_ids.to(device)\n",
    "    query_attention_masks = query_attention_masks.to(device)\n",
    "    query_labels = query_labels.to(device)\n",
    "    query_topics = query_topics.to(device)\n",
    "    query_sentiment_score = query_sentiment_score.to(device)\n",
    "    \n",
    "    output = model(query_input_ids, query_attention_masks, query_topics)\n",
    "    query_labels = query_labels.type(torch.float).unsqueeze(1)\n",
    "    output = torch.round(torch.sigmoid(output))  \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    1796 MB |    2386 MB |  247491 GB |  247490 GB |\\n|       from large pool |    1792 MB |    2357 MB |  239081 GB |  239080 GB |\\n|       from small pool |       3 MB |      30 MB |    8409 GB |    8409 GB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    1796 MB |    2386 MB |  247491 GB |  247490 GB |\\n|       from large pool |    1792 MB |    2357 MB |  239081 GB |  239080 GB |\\n|       from small pool |       3 MB |      30 MB |    8409 GB |    8409 GB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    1942 MB |    2594 MB |    2594 MB |  667648 KB |\\n|       from large pool |    1936 MB |    2556 MB |    2556 MB |  634880 KB |\\n|       from small pool |       6 MB |      38 MB |      38 MB |   32768 KB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  149338 KB |  291524 KB |  266598 GB |  266598 GB |\\n|       from large pool |  146577 KB |  283757 KB |  257638 GB |  257638 GB |\\n|       from small pool |    2761 KB |    8523 KB |    8959 GB |    8959 GB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     826    |    1064    |  108068 K  |  108067 K  |\\n|       from large pool |     304    |     452    |   64555 K  |   64555 K  |\\n|       from small pool |     522    |     614    |   43512 K  |   43511 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     826    |    1064    |  108068 K  |  108067 K  |\\n|       from large pool |     304    |     452    |   64555 K  |   64555 K  |\\n|       from small pool |     522    |     614    |   43512 K  |   43511 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      84    |     124    |     124    |      40    |\\n|       from large pool |      81    |     105    |     105    |      24    |\\n|       from small pool |       3    |      19    |      19    |      16    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      98    |     154    |   47031 K  |   47031 K  |\\n|       from large pool |      71    |     109    |   31971 K  |   31971 K  |\\n|       from small pool |      27    |      46    |   15060 K  |   15060 K  |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del test_input_ids\n",
    "del test_attention_masks\n",
    "del test_labels\n",
    "del test_topics\n",
    "del test_sentiment_score\n",
    "\n",
    "\n",
    "del train_input_ids\n",
    "del train_attention_masks\n",
    "del train_labels\n",
    "del train_topics\n",
    "del train_sentiment_score\n",
    "\n",
    "#test_input_ids = test_input_ids.to(device)\n",
    "# test_attention_masks = test_attention_masks.to(device)\n",
    "# test_labels = test_labels.to(device)\n",
    "# test_topics = test_topics.to(device)\n",
    "# test_sentiment_score = test_sentiment_score.to(device)\n",
    "\n",
    "# train_input_ids = train_input_ids.to(device)\n",
    "# train_attention_masks = train_attention_masks.to(device)\n",
    "# train_labels = train_labels.to(device)\n",
    "# train_topics = train_topics.to(device)\n",
    "# train_sentiment_score = train_sentiment_score.to(device)\n",
    "#\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "5753\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.65 GiB (GPU 0; 8.00 GiB total capacity; 6.70 GiB already allocated; 0 bytes free; 6.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m articles_with_same_topic \u001b[38;5;241m=\u001b[39m filter_artciles_with_same_topic(query, test_data)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(articles_with_same_topic))\n\u001b[0;32m----> 9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles_with_same_topic\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[1;32m     12\u001b[0m recommended_articles \u001b[38;5;241m=\u001b[39m filter_articles_based_on_predictions(articles_with_same_topic, predictions, RECOMMENDATION_SIZE)\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mpredict_label\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      8\u001b[0m query_topics \u001b[38;5;241m=\u001b[39m query_topics\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m query_sentiment_score \u001b[38;5;241m=\u001b[39m query_sentiment_score\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_attention_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m query_labels \u001b[38;5;241m=\u001b[39m query_labels\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(output))  \n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-22.12/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mBertPlusModel.forward\u001b[0;34m(self, input_ids, attention_mask, additional_input)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, additional_input):\n\u001b[0;32m----> 9\u001b[0m     _, pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_tuple()\n\u001b[1;32m     10\u001b[0m     concatenated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((pooled_output, additional_input\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(concatenated)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-22.12/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-22.12/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1014\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1022\u001b[0m     embedding_output,\n\u001b[1;32m   1023\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1032\u001b[0m )\n\u001b[1;32m   1033\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-22.12/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-22.12/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:238\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    236\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m    237\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 238\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-22.12/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-22.12/lib/python3.9/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-22.12/lib/python3.9/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.65 GiB (GPU 0; 8.00 GiB total capacity; 6.70 GiB already allocated; 0 bytes free; 6.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "RECOMMENDATION_SIZE = 5\n",
    "\n",
    "query = test_data.sample(n=1).reset_index(drop=True)\n",
    "\n",
    "print(get_topic_id(query))\n",
    "\n",
    "articles_with_same_topic = filter_artciles_with_same_topic(query, test_data)\n",
    "print(len(articles_with_same_topic))\n",
    "predictions = predict_label(articles_with_same_topic).squeeze(1)\n",
    "print(predictions)\n",
    "\n",
    "recommended_articles = filter_articles_based_on_predictions(articles_with_same_topic, predictions, RECOMMENDATION_SIZE)\n",
    "\n",
    "print(recommended_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f81b1a35a3bcb00987bde8d09f9263b6e52446a60f74d539393c43a25c83666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
